## #20210629

挺好的，正如你所知道的，上周我修订了我的手稿，在周五完成并再次提交了修订稿，希望能有一个好结果。

```
Fine, as you know, I revised my manuscript last week and finished and resubmitted the revised manuscript on Friday, hopefully with a good result.
```



---

首先，说一下硕士项目，我的意思是关于Benoit。他已经在real system测试了benchmark，我的意思是自适应PID，看起来不错。这是他的报告，目前只剩下应用部分还没完成，其他的都完成了。我不确定结果是好是怀，但是我觉得作为一个硕士生，在短短的4个月里，从零开始到现在，他做得已经足够好了，他从仿真建模，构建物理模型以及神经网络模型，然后测试了多种奖励函数，以及训练了大量的模型，我不得不承认他非常的努力。他今天来实验室在real system 上测试DDPG的效果，我的意思是仿真的效果还不错，但是在真实系统上的效果我还不知道，会议结束后我会去看看，然后尽量帮助他。对了，他需要为他的这个项目做一个poster吗？需要打印出来吗？

```
First, let's talk about the master project, I mean about Benoit. he has tested the benchmark in real system, I mean adaptive PID, and it looks good. Here is his report, so far only the implementation part is not finished yet, the rest is done. I'm not sure if the results are good or bad, but I think he has done well enough as a master student to start from scratch in just 4 months. He has modeled from simulation, built physical models and neural network models, and then tested different reward functions, so I have to admit that he has worked very hard. He came to the lab today to test the effect of DDPG on real system, I mean the effect of simulation is not bad, but I don't know the effect on real system yet, I will go to take a look after the meeting and try to help him. By the way, does he need to make a poster for this project? Does he need to print it out?
```

---

关于我的研究，正如你所看到的，这时我上周写的代码。我希望能够加快我的进度，所以上周末加班将之前的项目的DDPG和环境从spinningup转移到了Rllib，我的意思是我想将DDPG和其他一些先进的算法作为benchmark。不幸的是，我发现之前的项目的环境代码似乎有一些问题，关于初始化定义和模型评估都有一些问题。如你所看到的，这是之前的项目的代码，初始化定义的格式是错误的，区少了四个下划线，就像这样。另外关于episode的长度，一般来讲，我们不会在环境中直接设置，这是DDPG算法的一个超参，所以，如果在环境中设置了spisode的长度，那么训练次数将会比实际设定的多，模型评估速度也会慢，我不清楚为什么之前的项目可以正常运行，感觉很神奇。所以，基于之前的项目，我自己对环境重新进行了编程，就像这样。不过，还是存在一些问题，观测空间需要归一化，我发现计算所得的数值并没有完全被归一化，偶尔会报错，警示超出了空间。事实上，我已经设定了observation space的low和high 边界，你可以看这里，我不知道为什么，所以，你有什么建议吗？

```
About my research, as you can see, this is the code I wrote last week. I wanted to speed up my progress, so I worked overtime last weekend to transfer the DDPG and environment of the previous project from spinningup to Rllib, I mean I wanted to use the DDPG and some other state-of-the-art algorithms as benchmark. Unfortunately, I found that the environment code of the previous project seems to have some problems regarding both the initialization definition and the model evaluation.problems. 
As you can see, this is the code of the previous project and the initialization definition is in the wrong format, missing four underscores, like this.  
Also about the length of the episode, generally speaking, we don't set it directly in the environment, it's a hyperparameter of the DDPG algorithm, so if the length of the episode is set in the environment, then the number of training will be more than the actual setting, and the model evaluation will be slower, I'm not sure why the previous project works fine, it feels amazing. 
So, based on the previous project, I coded the environment again myself, just like this. However, there are still some problems, the observation space needs to be normalized, and I found that the calculated values are not completely normalized and sometimes an error is reported warning that the space is exceeded. In fact, I have set low and high bounds for the observation space, you can see them here, I don't know why, so, do you have any suggestions?

```

---

没关系，我想我还需要继续检查。

```
That is okay, I think I need to keep checking.
```

----

好的，我想我没有其他问题了，只要关于环境的问题解决了，我相信后续的问题都可以迎刃而解。

```
Okay, I think ...... I don't have any more questions, and once the problems about the environment are solved, I'm sure all the subsequent problems can be solved.
```













---



## #20210706

非常抱歉，上次周会，您问我关于Benoit的项目的问题，我没能很好的回答，我当时确实没能很好的理解您的问题。后来经过思考，他设计了三个控制器，分别是经典静态PID控制器，动态PID控制器和DDPG控制器，其中，他将经典静态PID控制器设置为benchmark，这个控制器的参数是固定的，另外两个控制器，我是说动态控制器和DDPG控制器本质上都是通过神经网络来对PID的权重在线实时的根据测量系统输出动态整定的。我想这是您上次的问题对吗？

```
I'm so sorry, last weekly meeting, you asked me about Benoit's project, I didn't answer well, I really didn't understand your question well at that time. Later, after thinking about it, he designed three controllers, a classical static PID controller, a dynamic PID controller and a DDPG controller, in which he set the classical static PID controller as benchmark, the parameters of this controller are fixed, and the other two controllers, I mean the dynamic controller and the DDPG controller are essentially neural networks to dynamically adjust the PID weights online in real time based on the output of the measurement system.  I think that was your last question, right?
```

目前他已经将仿真和实验都完成了，这是三个视频，可以明显的看到，benchmark最稳定，动态PID和DDPG振荡非常厉害，但是他们的MSE，我是说平均方差，其实他们的MSE比benchmark更好。我觉得他确实做了很多工作，尤其是仿真，他训练了一百多个强化学习模型，就像他的报告里面展示的一样，您知道的强化学习模型训练非常的耗时，他能做到这个程度，我觉不错，我的意思是很好。

```
He has now finished both the simulation and the experiment, these are the three videos, you can clearly see that benchmark is the most stable, dynamic PID and DDPG both have oscillation problems, but their MSE, I mean the mean squared error, actually their MSE is better than benchmark. I think he really did a lot of work, especially the simulation, he trained more than one hundred reinforcement learning models, as he showed in his report, you know, training the reinforcement learning models is very time-consuming, he can do this degree, I think it's not bad, I mean it's good.
```

当然，肯定也有作的不足的地方，他的报告写的还不够好，尤其是结构上还存在一些问题，我为他提供了一些建议，他还在修改，这是我给他的建议。另外我还邀请菲利普一起审阅他的报告。

```
Of course, there are certainly some shortcomings. His report is not well written, especially the structure still has some problems, and I provided him with some suggestions, and he is still revising it. In addition, I invited Philippe to review his report together.
```

关于我的研究，我上个月修订的那片手稿，我想你知道它的，我商周刚刚收到AST的录用通知，谢天谢地，经历了十八个月的艰难等待，终于可以稍微松口气。

```
Regarding my research, the manuscript I revised last month, I think you know about it, I just received an acceptance letter from AST last week, and thank goodness, after eighteen months of hard waiting, I'm finally relieved.
```

另外，关于Gyro，正如我上次跟你讲过的，之前的项目的环境模型确实存在问题，我自己重新编程写了一个环境模型，存在一点差异，我还在训练，我想今天晚上能出来结果。这是我把用RLlib的DDPG对Gyro进行控制，你可以看到，奖励函数似乎不收敛，另外，该模型是基于tensorflow，每当我用Pytorch来训练的时候总是出问题，所以我需要继续检查。因为我想用DDPG作为benchmark，所以，我想等我把DDPG的控制效果达到或者超过之前的项目后，我会马上开始调用基于模型的强化学习算法来对Gyro进行控制，并比较和分析他们。

```
Also, about Gyro, as I told you last time, the previous project did have problems with the environment model, I have reprogrammed myself to code an environment model, there is a little difference, I am still training and I think I will get the results this evening. This is the DDPG from RLlib that I put to control Gyro, as you can see, the reward function seems not to converge, also, the model is based on tensorflow and whenever I use Pytorch to train it always has problems, so I need to keep checking. Since I want to use DDPG as benchmark, so I think after I get the DDPG control up to or beyond the previous project, I will start applying model-based reinforcement learning algorithms to Gyro and compare and analyze them as soon as I can.
```



---

## # 20210713

挺好的，周日我和Tiger去了一趟ETHZ，看到了爱因斯坦用过的柜子，发现瑞士的大学都好漂亮。我的意思是，我很喜欢。

```
It´s good, Tiger and I went to ETHZ on Sunday and saw the cabinet Einstein used and found out that all the universities in Switzerland are so beautiful. I mean, I love it.
```

关于Benoit的项目，我和菲利普都给他提出了一些建议，但是他上周告诉我他已经开始army了，所以需要创造时间来修订报告，我昨天有提醒了他，我想他可以及时完成，我希望他可以。

```
About Benoit's project, Philippe and I both gave him some suggestions, but last week he told me he had started ARMY, so he needed to make time to revise the report, I did remind him yesterday again, I think he can finish it in time, I hope he can.
```

关于我的研究，上周，我主要做了两项工作，第一项是关于陀螺仪，我已经开始为陀螺仪测试基于模型的强化学习算法，但是还没有完成，这是我已经做的工作，还没有完成，并且存在一些问题，我将会检查并继续工作。另外关于benchmark，我将使用DDPG，目前也还在训练，还没有得到足够好的结果。这是一些训练得到的结果，所以，接下来我还需要继续为这些工作。

```
About my research, last week, I mainly worked on two projects, first one is about gyroscope, I have started testing model based reinforcement learning algorithm for gyroscope, but I haven't finished it yet, this is the work I have done, it is not finished yet and there are some problems, I will check and continue working on it. Also about benchmark, I will use DDPG, which is also still training and not yet getting good enough results. These are some of the results I got from the training, so I need to continue working for those next.
```

另一项工作是，关于基于迁移学习的热物理建模，这里我采用的是基于模型的迁移学习，这是一个目前应用最为广泛的迁移学习方法，效果也很好。这是我在中国的一个博士项目，我想我会在一个月内完成这个项目。并且我想这个成果对于航天器热控系统领域是一个很重要的贡献，我想它是的，至少我觉得这对于未来的深空探测，以及AI在航天器控制中的应用具有很大的推动作用。当然，对于机器人系统的建模也具有参考意义。等我在航天器系统上完成验证后，我会测试它在陀螺仪上的效果。

```
The another work is about thermal physics modeling based on transfer learning, here I am using model-based transfer learning, which is one most widely used transfer learning method and works well. It is my one PhD project in China, and I think I will finish this project within one month. And I think the results are a very important contribution to the spacecraft thermal control field, and I think it is, or at least I think it is a big promotion for the future of deep space exploration, and the application of AI in spacecraft control. Of course, I think it's also useful for modeling robotic systems. I will test it on gyroscopes when I finish validating it on spacecraft systems.
```

## #20210720

上周的主要工作集中在超参数的优化，就像我昨天说的，我在为我的超参数优化编程。为什么我要做这个优化的工作，你知道的，调节超参数是一个很繁琐且耗时的过程，为了提高效率，所以我必须用一种科学的高效的优化方法。我可以简单给你展示一下我今天要讲的内容。

```
Last week my main work was focused on hyperparameter optimization, as I said yesterday, I was coding my hyperparameter optimization. Why I'm doing this optimization work, you know, tuning hyperparameters is a very complicated and time consuming process, in order to increase efficiency, so I have to use a scientific and efficient optimization method. I can briefly show you what I'm going to talk about today.
```

这是之前为陀螺仪做的在不同奖励函数下基于rllib的DDPG的结果，你可以看到目前奖励函数效果都不是很好，并且与之前的项目存在较大的差异，我对奖励函数以及超参数做了很多的优化和调整，效果都不是很好。于是，我想还一种方法来优化它，于是我开始尝试使用ray.tune，这是一种来自UCB为机器学习设计的分布式计算框架，我上周一直忙于将ray.tune应用到我的模型中，目前我已经得到了一些初步的结果，效果还行。

```
This is the result of DDPG based on rllib with different reward functions for gyroscope, you can see that the reward functions are not very good so far, and there are big differences with the previous project, I have done a lot of optimization and modification of the reward functions and hyperparameters, but the results are not very good. So I wanted to optimize it in another way, so I started to try using ray.tune, a distributed computing framework designed for machine learning from UCB. I have been busy applying ray.tune to my model last week, and I have got some preliminary results so far, and the results are good.
```

你可以从这个图看到，基于ray.yune，我利用XXX算法对这些超参数进行优化，比如每一层的神经元个数，激活函数类型，学习率以及batch_size。优化的过程简单说就是，首先基于XXX算法抽样出100组超参数样本，然后对他们进行并行计算，对于一些效果不好的样本，系统会提前终止，并为更有希望的样本分配更多的时间和资源。基于这种方法，我们可以在调整超参数上大大提高效率。

```
You can see from this graph that based on ray.yune, I use one optimization algorithm to optimize these hyperparameters, such as the number of neurons for each layer, the activation function type, the learning rate and the batch_size. And the system will terminate the trails early and assign more time and resources to the more promising trails. In this way, we can greatly improve the efficiency in tuning hyperparameters.
```

让我直接来看结果。我们从这里可以看到，大部分的很差的试验都被提前终结了，而这条线一直处于持续上升的趋势，因此，到目前为止，经过100次的试验，这个试验是最好的，不过整个程序还不完美，我希望继续训练，看一看更长的训练是否会有不同的结果。所有我的代码和经验我都会分享在我的github上面，如果有学生对这方脉年感兴趣，可以随时访问，希望对他们有帮助。

```
Let's look at the results directly. We can see from here, most of the very poor trials were terminated early, and this line has been on a continuous rising trend, so far, after 100 trials, this trial is the best, but the whole program is not perfect, and I hope to continue training to see if longer training will have different results.

In addition, I am also working on my PhD project in China at the same time, one novel method for building a surrogate model for spacecraft using deep learning and transfer learning, which can be applied to a different working conditions and environments, and the final testing and validation will be completed in 1 or 2 weeks.

Later I will also publish one new paper, and I will open my source code and data for this project. And I men, I hope I can contribute a little bit to the application of AI in spacecraft.
```













